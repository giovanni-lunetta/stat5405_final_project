---
title: The Application of Machine Learning to Predict NFL Running Back Performance 
subtitle: STATS-5405
author:
  - name: Giovanni Lunetta
    email: giovanni.lunetta@uconn.edu
    affiliations: 
        - id: UConn
          name: University of Connecticut
          department: Statistics
          address: 2075 Hillside Road
          city: Storrs
          state: Connecticut
          postal-code: 06269
    attributes:
        corresponding: true
  - name: Sam Lutzel
    email: samuel.lutzel@uconn.edu
    affiliations: 
        - id: UConn
          name: University of Connecticut
          department: Statistics
          address: 2075 Hillside Road
          city: Storrs
          state: Connecticut
          postal-code: 06269
    attributes:
        corresponding: true
abstract: |
  This study employs an ensemble of machine learning techniques, including Multiple Linear Regression (MLR), Generalized Linear Models (GLM), and XGBoost to enhance predictive analytics in the National Football League (NFL). The research focuses on one of the most critical aspects of football offense - the running game. In order to predict the yards gained by NFL running backs following a handoff, the study analyzes a plethora of player tracking data from the 2017 to 2019 seasons. It integrates a variety of factors such as player positions, orientation, and the situational context of the game, including down, distance, and field position. This multifaceted approach aims to yield highly accurate models that can serve as valuable tools for coaching staffs to optimize play-calling, manage player workloads, and enhance game planning. The predictive insights derived from this research are intended to support teams in deploying their running backs more effectively, leading to potentially improved outcomes on the field. As the NFL continues to evolve with a greater emphasis on analytics, this study seeks to contribute significantly to the field of sports analytics by providing a model that underscores the importance of the running game in a predominantly pass-oriented league.
keywords: 
  - Machine Learning
  - Predictive Analytics
  - Sports Analytics
date: December 2023
bibliography: bibliography.bib
format:
  elsevier-pdf:
    keep-tex: true
    journal:
      name: Journal Name
      formatting: preprint
      model: 3p
      cite-style: super
---

# Introduction

## Background of the National Football League

The National Football League (NFL) is the most popular professional sports league in the United States, with an estimated 187 million fans. The league consists of 32 teams divided into two conferences, the American Football Conference (AFC) and the National Football Conference (NFC). Each conference is further divided into four divisions, with four teams in each division. The NFL season consists of 17 weeks, with each team playing 16 games and having one bye week. The regular season is followed by a 12-team playoff, with the winner of each conference advancing to the Super Bowl, the league's championship game.

The NFL game strategy primarily consists of two ways to advance the football down the field: running and passing. Running the ball is defined as a play in which the quarterback gives the ball to another player that is located behind them (a handoff or a small toss). The player then attempts to run the ball down the field as far as possible before being tackled to the ground. Passing the ball is defined as a play in which the quarterback throws the ball to another player down the field. The player then attempts to catch the ball and advance it down the field as far as possible before being tackled to the ground. The goal of each team is to score as many points as possible by advancing the ball down the field and into the end zone or by kicking the ball through the field goal posts. The team with the most points at the end of the game wins.

## Motivation

The primary goal of this research is to develop cutting-edge predictive models capable of accurately estimating the yards a running back will gain after a handoff during NFL games. This objective is pivotal for formulating advanced offensive strategies and refining player evaluations. The running play is a fundamental aspect of the game that can dictate the tempo, control the clock, and establish physical dominance.

The motivation behind this study stems from the transformative impact that data analytics has had on sports, particularly in the NFL, where the fusion of technology and sports science has begun to redefine how the game is played and understood. In an era where marginal gains are increasingly sought after, the ability to predict the outcome of running plays with high precision can provide teams with a significant competitive advantage. It enables coaches to make informed decisions regarding play selection, player rotations, and game management, especially in critical moments of a match. Additionally, the insights from this study can empower front offices in their scouting and drafting processes by quantifying the expected value a running back adds to their team. Furthermore, there is also a tremendous opportunity to leverage these predictive models on the defensive end of the ball, allowing teams to better anticipate and defend against running plays. With better insights into the opponents running game, defenses can adjust their schemes and personnel to counter the opposing team's offensive strategy, such as by stacking the box or blitzing the quarterback. In addition to the benefits performance analytics provides to the teams, it also helps fans select better fantasy football teams and make more informed betting decisions. This leads to a more engaging and enjoyable experience for the fans, which is critical for the long-term success of the league. 

Ultimately, the true beauty of this research lies in its ability to bridge the gap between complex player tracking data and practical on-field strategies. In specific, it's about enhancing the very essence of the game and enriching the broader discourse on sports performance analytics. By pioneering research in this domain, this study is set to propel the analytical capabilities of NFL teams to new heights, providing them with tools that were once considered unimaginable. Ultimately, it contributes to the ongoing evolution of the sport itself, marking a pivotal moment in the history of football and the broader world of sports analytics. 

# Methods

## Data Cleaning and Preprocessing

Data cleaning and preprocessing are critical to ensuring the quality and integrity of machine learning models. The cleaned dataset was obtained by addressing any inconsistencies and handling missing data through imputation techniques. Preprocessing steps included encoding categorical variables using one-hot encoding or label encoding methods and scaling and normalizing numerical variables to ensure they are on comparable scales. 

To begin, several variables were removed in order to simulate the beginning of each play. For instance, the dataset include the speed, acceleration, direction, and distance traveled of each player on the field at the beginning of each play. However, this information is not available to the coaching staff prior to the snap of the ball. Therefore, these variables were removed from the datset. Furthermore, some variables may introduce multicollinearity issues since they represent the same information. For instance, the dataset includes the name and id of each player on the field. However, these variables are highly correlated with each other. Therefore, only one of these variables was kept in the dataset.

When the dataset was first obtained, each row represented a single player on the field. However, the goal of this study is to predict the yards gained after a handoff for each play. Therefore, every 22 rows had to be merged into one row (11 players on each side). During this process, any duplicated information/columns were dropped from the dataset. One example of this is the weather during the play. When all 22 rows are merged, it included the weather 22 times, even though weather is only needed once. 

In addition to the attempt at removing unnecessary variables and proactively removing potenital multicollienarity issues, variables such as weather, wind speed, and wind direction had multiple levels, some of which represented the same data. For instance, "rainy" and "showers" were considered the same weather type. There were multiple instances of relationships between inputs on the same variable that existed similar to the example above. Due to this, these weather types were grouped into one category - "rain." This helps to drastically reduce the complexity of the dataset. These variables were also converted to factors for the modeling process.

Variables such as offense personnel had to be converted to dummy variables. The input for each dummy variable was either 0 or 1 depending on whether or not the offense had that personnel on the field. For instance, if the offense had 2 running backs on the field, the input for the "RB" dummy variable would be 2. The input for the "WR" dummy variable would be 0 since there were no wide receivers on the field. This process was repeated for all personnel types. 

One additional step that was taken was the randomization of all plays within the dataset. The reason for this randomization is to ensure that each row (play of a game) is independent of one another. Furthermore, the dataset only contains running plays. Therefore, passing plays that occured between the running plays were removed. In other words, all plays are not dependent on the previous outcome. In all, the randomization of the dataset in conjunction with the removal of passing plays ensures that each row is independent of one another.

## Data Exploration

Prior to the development of the predictive models, a comprehensive exploratory data analysis (EDA) was conducted to understand the underlying structure and characteristics of the data. This step involved visualizing the distribution of key variables, identifying patterns and outliers, and exploring correlations and interactions between predictors. Data visualization, through techniques like scatter plots, histograms, and heatmaps, can offer an intuitive understanding of data distributions, correlations, and potential clusters within the dataset. EDA-informed feature selection and engineering strategies highlighted potential predictors that are most informative of yards gained after a handoff.

## Feature Engineering
During preprocessing, we will also undertake feature engineering to create new variables that may have a stronger relationship with the target variable. This could include interaction terms that capture the combined effect of two predictors, polynomial features for capturing non-linear relationships, and domain-specific features that encapsulate strategic elements of the game.

## Model Development and Evaluation

With a clean and prepared dataset, we will then proceed to develop our MLR, GLM, and XGBoost models.

## Multiple Linear Regression (MLR)

Multiple linear regression models predict a continuous repsonse variable using a linear combination of predictors. 
For our baseline MLR model, we will use the ordinary least squares (OLS) method to estimate the coefficients of our predictor variables. The model is specified as:

$Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \ldots + \beta_jX_{ij} + \epsilon_i$

where $Y_i$ represents the yards gained after the handoff for the $i^{th}$ observation, $X_{ij}$ is the $i^{th}$ observation on the $j^{th}$ predictor variable (where $j = 1, ..., p$),\ $\beta_j$ is the $j^{th}$ coefficient to be estimated corresponding to the proper $X_{ij}$ variable, and $\epsilon_i$ is the error term. The $j^{th}$ coefficient, $\beta_j$, represents the change in the response variable for a unit change in the $X_{ij}$ predictor variable, while holding all other predictors constant. 

The ordinary least squares (OLS) method will be deployed to minimize the sum of the squared differences between the observed and predicted values. The optimization problem can be represented as minimizing the sum of errors squared:

$$
\Sigma_{i=0}^n \epsilon_i^2 = \Sigma_{i=0}^n (Y_i - \hat{Y}_i)^2 = \Sigma_{i=0}^n (Y_i - \beta_0 - \beta_1X_{i1} - \ldots - \beta_jX_{ij})^2
$$

Here, $\Sigma_{i=0}^n \epsilon_i^2$ represents the sum of the squared residuals, which we aim to minimize. The variable $\hat{Y}_i$ represents the predicted outcome of the model for the $i^{th}$ observation. This approach assumes that the relationship between the independent variables and the dependent variable is linear. To ensure the robustness of our model, we will conduct a series of diagnostic tests:

1. Linearity: We will use scatter plots and residual plots to verify that the relationship between the predictors and the response is linear.
2. Homoscedasticity: We will inspect the residuals to confirm constant variance across all levels of the independent variables. This can be assessed visually using a residual vs. fitted values plot.
3. Independence: The Durbin-Watson test will help in detecting the presence of autocorrelation in the residuals, which should not be present in the data.
4. Normality of Residuals: Normality will be checked using Q-Q plots and statistical tests like the Shapiro-Wilk test.

If any assumptions are violated, we may consider transformations of variables or the use of robust regression techniques.

## Generalized Linear Models (GLM)
GLMs extend the linear model framework to allow for response variables that have error distribution models other than a normal distribution. They are particularly useful when dealing with non-normal response variables, such as count data or binary outcomes. In its general form, a GLM consists of three elements:

1. Random Component: Specifies the probability distribution of the response variable $Y$, such as normal, binomial, Poisson, or exponential.
2. Systematic Component: A linear predictor $\eta = X\beta$.
3. Link Function: A function $g$ that relates the mean of the response variable $E(Y)$ to the linear predictor $\eta$.

The choice of link function is crucial and is typically selected based on the nature of the distribution of the response variable. For instance, a logit link function is used for a binomial distribution, and a log link function is often used for a Poisson distribution.

The likelihood function for a GLM can be written as:

$$
L(\beta) = \prod_{i=1}^{n} f(y_i; \theta_i, \phi)
$$

where $f(y_i; \theta_i, \phi)$ is the probability function for the $i$-th observation, $\theta_i$ is the parameter of interest (e.g., mean), and $\phi$ is the dispersion parameter. The goal is to find the values of $\beta$ that maximize this likelihood function.

## XGBoost
XGBoost stands for eXtreme Gradient Boosting and is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. It is a powerful technique that can handle a variety of regression and classification problems. For regression, it can be configured to optimize for different loss functions; the most common for regression being the squared error loss:

$$
L(\theta) = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$

where $y_i$ are the observed values, and $\hat{y}_i$ are the predicted values.

In XGBoost, each new tree is built to correct the errors made by the previous ones. The algorithm combines weak predictive models to form a strong predictor. The model's complexity is controlled by the regularization term $\Omega(\theta)$ which is a function of the tree structure and the number of leaves. The overall objective function to be minimized is:

$$
\text{Obj}(\theta) = L(\theta) + \lambda \sum_{k}(w_k^2) + \gamma T
$$

where $w_k$ represents the leaf weights of the trees, $T$ is the number of leaves, $\lambda$ is the L2 regularization term on the weights, and $\gamma$ is the complexity control on the number of leaves. For regression tasks, we can also utilize the quantile loss which is particularly useful for prediction intervals:

$$
L_{\tau}(\theta) = \sum_{i=1}^{n} \left[ \tau (y_i - \hat{y}_i) \mathbb{1}_{y_i \geq \hat{y}_i} + (1 - \tau) (\hat{y}_i - y_i) \mathbb{1}_{y_i < \hat{y}_i} \right]
$$

Here, 1 is an indicator function, and $\tau$ is the quantile of interest, allowing us to model different parts of the conditional distribution of the response variable.

XGBoost also provides a feature importance score, which is a metric that quantifies the contribution of each feature to the model's predictive power. This is done by measuring the impact on the model's accuracy each time a feature is used to split the data across all trees.

## Model Performance Metrics

For MLR and GLM, the model's performance will be evaluated using metrics such as the Mean Squared Error (MSE) and the Mean Absolute Error (MAE). For the XGBoost model, along with MSE and MAE, we will assess performance using additional metrics like the R-squared for regression tasks and feature importance scores to understand which variables are most predictive.

## Model Interpretation and Application

The final step will be to interpret the models in the context of NFL games. This will involve translating the statistical outputs into actionable insights for coaches and team strategists, providing recommendations on how to leverage the results for competitive advantage in play-calling and player evaluation.

## Software and Tools

All analyses will be conducted in R, a statistical computing language that provides a wide array of packages for machine learning and data analysis. For MLR and GLM, we will utilize the `stats` package that comes with the R base installation. For our XGBoost model, we will use the `xgboost` package, which is specifically designed for speed and performance. Data manipulation and cleansing will be managed with packages like `dplyr` and `tidyr`, while `ggplot2` will be employed for data visualization to facilitate understanding and interpretation of the data and model outputs. For feature engineering and preprocessing, we will take advantage of `caret` or `recipes`. Hyperparameter tuning can be optimized using the `tune` package, and for cross-validation, the `rsample` package will be employed. The `broom` and `modelr` packages will be useful for tidying model outputs and working with models in a pipeline, respectively. This suite of packages will enable a comprehensive workflow within R for developing, evaluating, and interpreting the predictive models.

## Model Development

To commence our analytical journey, we initiate with Multiple Linear Regression (MLR) as our foundational modeling technique. This approach is not just a stepping stone, but a critical phase in our analysis, offering valuable insights into the relationships between various features of the game and the yards gained. MLR helps discern the linear associations and relative importance of different variables, setting the stage for more complex modeling. Subsequent to this exploratory analysis, we transition to XGBoost, an advanced machine learning technique renowned for its predictive power and efficiency. XGBoost, a gradient boosting framework, is chosen for its ability to handle the dataset's complexity, non-linear relationships, and interactions among variables more adeptly. This shift from MLR to XGBoost embodies our methodological progression, from understanding the foundational relationships in our data to harnessing advanced computational techniques for more accurate and robust predictions in the dynamic and unpredictable context of NFL games.

# Multiple Linear Regression Model

First, lets start by importing the completely clean and ready to use dataset. This dataset was the result of procedures outlined in the data cleaning and preprocessing section.

```{r}
# Import the dataset
football_data <- read.csv("/Users/samlutz10/Desktop/STAT5405/Final Project/train_ready.csv")

str(football_data)
```

Since we are only focused on the most common plays and not breakout runs, we will remove any data of 10 yards or more. This is due to the fact that plays resulting in 10 yards are considered to be just as successful are play resulting in more than 10 yards. In addition, any plays resulting in 0 or negative yards will be removed. This is due to the fact that these plays are considered to be unsuccessful. 
  
```{r}
# Remove any rows whose yards column is greater than 30
football_data <- football_data[football_data$Yards <= 10 & football_data$Yards > 0, ]
```

Lets now look at the histogram of Yards to get a better understanding of the distribution of the response variable.

```{r}
# Histogram of Yards
hist(football_data$Yards, main = "Histogram of Yards", xlab = "Yards", ylab = "Frequency", col = "blue")
```

Based on the histogram above, it is apparent that the number of yards is skewed to the right. In order to address this issue, the Box-Cox transformation will be utilized. This will help to normalize the distribution of the response variable. The Box-Cox transformation can transform non-normal data into normal data. 

```{r}
library(MASS)
lambda_null <- boxcox(lm(Yards ~ 1, data = football_data))
```

The best lambda value for the Box-Cox tranformation is as follows:

```{r}
lambda_null$x[which.max(lambda_null$y)]
```

Now the Box-Cox tranformation will be applied to the variable Yards with the best lambda value.

```{r}
lm_model_null <- lm((Yards^(lambda_null$x[which.max(lambda_null$y)])) ~ 1, data = football_data)
summary(lm_model_null)
```

Now the full model using all of the predictors will be created. In this full model, there will be 0 interactions between the predictors. The same process using the Box-Cox transformation will be used to normalize the distribution of the response variable.

```{r}
lambda_full <- boxcox(lm(Yards ~ ., data = football_data))
```

The best lambda value for the Box-Cox tranformation is as follows:

```{r}
lambda_full$x[which.max(lambda_full$y)]
```

Now the Box-Cox tranformation will be applied to the variable Yards with the best lambda value.

```{r}
lm_model_full <- lm((Yards^(lambda_full$x[which.max(lambda_full$y)])) ~ ., data = football_data)
summary(lm_model_full)
```

Based on the results of the full model, the model shows which predictors are statistically significant. The most significant predictors are Distance, DefendersInTheBox, and YardsFromOwnGoal. Creating scatter plots of the response variable versus each of these predictors will help to gain a better understanding of how these variables affect the predicted yards. 

```{r}
# Scatter plot of yards vs. Distance
plot(football_data$Yards, football_data$Distance, main = "Yards vs. Distance", xlab = "Yards", ylab = "Distance", pch = 19)
```

Looking at this scatter plot of Yards vs Distance, there appears to only be a sinusoial relationship between the two variables. Therefore, when a reduced model is created, a sine function will be used to model the relationship between the two variables.

```{r}
# Scatter plot of yards vs. Defenders In The Box
plot(football_data$Yards, football_data$DefendersInTheBox, main = "Yards vs. Defenders In The Box", xlab = "Yards", ylab = "Defenders In The Box", pch = 19)
```

Looking at this scatter plot of Yards vs Defenders In The Box, there appears to be a quadratic function relationship between the two variables. Therefore, when a reduced model is created, the Defenders In The Box variable will be squared in order to capture this relationship

```{r}
# Scatter plot of yards vs. YardsFromOwnGoal
plot(football_data$Yards, football_data$YardsFromOwnGoal, main = "Yards vs. Yards From Own Goal", xlab = "Yards", ylab = "Yards From Own Goal", pch = 19)
```

The scatterplot of Yards vs Yards From Own Goal shows that there is a linear relationship between the two variables. Therefore, when a reduced model is created, the Yards From Own Goal variable will not be changed since the model is already capturing this linear relationship.

In order to capture these relationships between variables, a new, full model will be synthesized. This model will include the Distance variable with a sine function and the Defenders In The Box variable squared. However, the Box-Cox transformation will need to be run again.

```{r}
lambda_full <- boxcox(lm(Yards ~ Quarter + Down + sin(Distance) + OffenseFormation + RB + TE + WR + DefendersInTheBox^2 + Week + GameWeather + Temperature + Humidity + WindSpeed + WindDirection + GameHour + DL + LB + BL + YardsFromOwnGoal + ScoreDelta, data = football_data))
summary(lambda_full)
```

The best lambda value for the Box-Cox tranformation is as follows:

```{r}
lambda_full$x[which.max(lambda_full$y)]
```

Now the Box-Cox tranformation will be applied to the variable Yards with the best lambda value.

```{r}
lm_model_full <- lm((Yards^(lambda_full$x[which.max(lambda_full$y)])) ~ Quarter + Down + sin(Distance) + OffenseFormation + RB + TE + WR + DefendersInTheBox^2 + Week + GameWeather + Temperature + Humidity + WindSpeed + WindDirection + GameHour + DL + LB + BL + YardsFromOwnGoal + ScoreDelta, data = football_data)
summary(lm_model_full)
```

Now lets use the step function to find the best model for the data.

```{r}
lm_model_step <- invisible(step(lm_model_full, direction = "backward"))
summary(lm_model_step)
```

This new reduced model needs to be run similar to that of the previous models utilizing the Box-Cox transformation. 

```{r}
lambda_reduced <- boxcox(lm(Yards ~ sin(Distance) + WR + DefendersInTheBox^2 + Temperature + BL + YardsFromOwnGoal, data = football_data))
```

The best lambda value for the Box-Cox tranformation is as follows:

```{r}
lambda_reduced$x[which.max(lambda_reduced$y)]
```

Now the Box-Cox tranformation will be applied to the variable Yards with the best lambda value.

```{r}
lm_model_reduced <- lm((Yards^(lambda_reduced$x[which.max(lambda_reduced$y)])) ~ sin(Distance) + WR + DefendersInTheBox^2 + Temperature + BL + YardsFromOwnGoal, data = football_data)
summary(lm_model_reduced)
```

By creating a Residuals vs Fitted plot, a Q-Q Residuals plot, a Scale-Location plot, and a Residuals vs Leverage plot, the assumptions of the model can be checked. 
```{r}
par(mfrow = c(2, 2))
old.par = par(mar = c(3, 4, 1, 2))
plot(lm_model_reduced)
```

Based on the plots above, the normality assumption is not violated, as the Q-Q Residuals plot shows that the residuals are mainly normally distributed. 

In order to gain a better understanding of how these variables affect the predicted yards, scatter plots of the predicted yards versus each of the predictors will be created. However, the only scatter plots that will be synthesized are those that have the three highest t values: Distance, DefendersInTheBox, and YardsFromOwnGoal. 

```{r}
football_data$predicted_yards <- predict(lm_model_reduced, football_data)
```

```{r}
# Scatter plot of predicted yards vs. Distance
plot(football_data$predicted_yards, football_data$Distance, main = "Predicted Yards vs. Distance", xlab = "Predicted Yards", ylab = "Distance", pch = 19)
abline(0, 1, col = "red")  # Adds a 45-degree line
```

```{r}
# Scatter plot of predicted yards vs. DefendersInTheBox
plot(football_data$predicted_yards, football_data$DefendersInTheBox, main = "Predicted Yards vs. DefendersInTheBox", xlab = "Predicted Yards", ylab = "DefendersInTheBox", pch = 19)
abline(0, 1, col = "red")  # Adds a 45-degree line
```

```{r}
# Scatter plot of predicted yards vs. YardsFromOwnGoal
plot(football_data$predicted_yards, football_data$YardsFromOwnGoal, main = "Predicted Yards vs. YardsFromOwnGoal", xlab = "Predicted Yards", ylab = "YardsFromOwnGoal", pch = 19)
abline(0, 1, col = "red")  # Adds a 45-degree line
```

# GLM with Gamma Response

The next model that will be synthesized is a GLM with a gamma response. The gamma distribution is a two-parameter family of continuous probability distributions. The gamma distribution is a generalization of the exponential distribution. The gamma distribution is frequently used to model right-skewed data. The gamma distribution is a two-parameter family of continuous probability distributions. The gamma distribution is a generalization of the exponential distribution. The gamma distribution is frequently used to model right-skewed data. The gamma distribution is a two-parameter family of continuous probability distributions. The gamma distribution is a generalization of the exponential distribution. The gamma distribution is frequently used to model right-skewed data. The gamma distribution is a two-parameter family of continuous probability distributions. The gamma distribution is a generalization of the exponential distribution. The gamma distribution is frequently used to model right-skewed data. The gamma distribution is a two-parameter family of continuous probability distributions. The gamma distribution is a generalization of the exponential distribution. The gamma distribution is frequently used to model right-skewed data. The gamma distribution is a two-parameter family of continuous probability distributions. The gamma distribution is a generalization of the exponential distribution. The gamma distribution is frequently used to model right-skewed data.

Lets start by removing any rows whose yards column is negative. This is because the gamma distribution cannot have negative values. 

```{r}
# Remove any rows whose yards column is negative
football_data <- football_data[football_data$Yards >= 0.1, ]
football_data <- football_data[football_data$Yards <= 10, ]
```

```{r}
# Fit the model
glm_model_gamma <- glm(Yards ~ ., data = football_data, family = Gamma(link = "log"))
```

Now lets use the step function to find the best model for the data.

```{r}
#glm_model_step <- step(glm_model_gamma, direction = "backward")
# summary(glm_model_step)
```

Running this reduced model yields the following results:

```{r}
reduced_model <- glm(Yards ~ YardsFromOwnGoal + Distance + RB + WR + DefendersInTheBox + BL, data=football_data, family = Gamma(link = "log"))
summary(reduced_model)
```

By creating a Residuals vs Fitted plot, a Q-Q Residuals plot, a Scale-Location plot, and a Residuals vs Leverage plot, the assumptions of the model can be checked. 
```{r}
par(mfrow = c(2, 2))
old.par = par(mar = c(3, 4, 1, 2))
plot(glm_model_gamma)
```

# XGBoost Model

Moving now into the XGBoost model, the data was split into a training set and a testing set in order to evaluate performance metrics. The training set will be used to train the model, while the testing set will be used to test the model. The training set will be 80% of the data, while the testing set will be 20% of the data and randomly selected.

```{r}
library(fastDummies)

# Convert specified factor columns to dummy variables
football_data <- dummy_cols(football_data, 
                            select_columns = c("WindDirection", "GameWeather", "OffenseFormation"), 
                            remove_selected_columns = TRUE)

# Split the data into a training set and a testing set
set.seed(123457)
train.prop <- 0.80
strats <- football_data$Yards
rr <- split(1:length(strats), strats)
idx <- sort(as.numeric(unlist(sapply(rr, 
        function(x) sample(x, length(x)*train.prop)))))
train.set <- football_data[idx, ]
test.set <- football_data[-idx, ]
```

 The average of yards in both the training and testing sets are computed to ensure they are about the same. In addition, both of those should be similar to that of the original dataset. This process will help to ensure that the training and testing sets are representative of the entire dataset. 

```{r}
# Check that the average of yards is about the same in both the training and testing sets
(ave_train <- mean(train.set$Yards))
(ave_test <- mean(test.set$Yards))
(ave_all <- mean(football_data$Yards))
```

Based on the results above, the average yards is about the same in both the training and testing sets. In addition, both of those are similar to that of the original dataset. 

Now that the data has been split into a training set and a testing set, development of the XGBoost model can begin.

```{r}
# Load the xgboost package
library(xgboost)

# Prepare the data for XGBoost
# Convert data to matrix format as xgboost works with the matrix
train_matrix <- as.matrix(train.set[, -which(names(train.set) == "Yards")])
test_matrix <- as.matrix(test.set[, -which(names(test.set) == "Yards")])

# Create DMatrices
dtrain <- xgb.DMatrix(data = train_matrix, label = train.set$Yards)
dtest <- xgb.DMatrix(data = test_matrix, label = test.set$Yards)

# Set XGBoost parameters
params <- list(
    booster = "gbtree",
    objective = "reg:squarederror",  # Objective function for regression
    eta = 0.1,                      # Learning rate
    max_depth = 6,                  # Depth of trees
    subsample = 0.5,                # Subsampling of the training data
    colsample_bytree = 0.5          # Subsampling of features
)

# Number of boosting rounds
nrounds <- 100

# Train the model
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = nrounds)

# Predicting
xgb_predictions <- predict(xgb_model, dtest)

true_values <- test.set$Yards
```

A scatter plot of the actual versus predicted values can provide a clear visual indication of how well the model is performing:
```{r}
# Scatter plot of actual vs. predicted values
plot(true_values, xgb_predictions, main = "Actual vs Predicted Yards", xlab = "Actual Yards", ylab = "Predicted Yards", pch = 19)
abline(0, 1, col = "red")  # Adds a 45-degree line
```

As we can see, the model is performing well, but fails to predict carries over 20 yards. This is likely due to the fact that there are very few carries over 20 yards in the dataset. We can check the summary statistics of yards to confirm this.
```{r}
summary(football_data$Yards)
```

75% of the carries 6 yards or less, while 50% of the carries are 3 yards or less. This is likely the reason why the model is failing to predict carries over 20 yards.

Our goal is to determine what plays a team should run on, therefore, because over 75% of our data consists of carries 6 yards or less and our model fails to predict a run of over 20 yards, we will remove all carries over 20 yards from the dataset. This will help to ensure that the model is predicting the majority of the carries in the dataset.

```{r}
# Remove all carries over 20 yards
football_data <- football_data[football_data$Yards <= 20, ]
```

Now that the data has been cleaned, we can re-run the XGBoost model:
```{r}
# Split the data into a training set and a testing set
set.seed(123457)
train.prop <- 0.80
strats <- football_data$Yards
rr <- split(1:length(strats), strats)
idx <- sort(as.numeric(unlist(sapply(rr, 
        function(x) sample(x, length(x)*train.prop)))))
train.set <- football_data[idx, ]
test.set <- football_data[-idx, ]

# Check that the average of yards is about the same in both the training and testing sets
(ave_train <- mean(train.set$Yards))
(ave_test <- mean(test.set$Yards))
(ave_all <- mean(football_data$Yards))

# Load the xgboost package
library(xgboost)

# Prepare the data for XGBoost
# Convert data to matrix format as xgboost works with the matrix
train_matrix <- as.matrix(train.set[, -which(names(train.set) == "Yards")])
test_matrix <- as.matrix(test.set[, -which(names(test.set) == "Yards")])

# Create DMatrices
dtrain <- xgb.DMatrix(data = train_matrix, label = train.set$Yards)
dtest <- xgb.DMatrix(data = test_matrix, label = test.set$Yards)

# Set XGBoost parameters
params <- list(
    booster = "gbtree",
    objective = "reg:squarederror",  # Objective function for regression
    eta = 0.1,                      # Learning rate
    max_depth = 6,                  # Depth of trees
    subsample = 0.5,                # Subsampling of the training data
    colsample_bytree = 0.5          # Subsampling of features
)

# Number of boosting rounds
nrounds <- 100

# Train the model
xgb_model <- xgb.train(params = params, data = dtrain, nrounds = nrounds)

# Predicting
xgb_predictions <- predict(xgb_model, dtest)

# Evaluate the model
# For example, using Root Mean Squared Error (RMSE)
true_values <- test.set$Yards
rmse <- sqrt(mean((true_values - xgb_predictions)^2))
print(paste("RMSE:", rmse))
```

Before commenting on the results, lets take a look at some plots to get a better understanding of the model's performance.
```{r}
# Scatter plot of actual vs. predicted values
plot(true_values, xgb_predictions, main = "Actual vs Predicted Yards", xlab = "Actual Yards", ylab = "Predicted Yards", pch = 19)
abline(0, 1, col = "red")  # Adds a 45-degree line
```
```{r}
# Variable importance plot
importance_matrix <- xgb.importance(feature_names = colnames(train_matrix), model = xgb_model)
xgb.plot.importance(importance_matrix)
```

```{r}
# Mean Absolute Error
mae <- mean(abs(true_values - xgb_predictions))
print(paste("MAE:", mae))
```

```{r}
# Residuals plot
residuals <- true_values - xgb_predictions
plot(residuals, type = "l", main = "Residuals Plot", xlab = "Index", ylab = "Residual")
abline(h = 0, col = "red")
```










# References {-}


# Bibliography styles

Here are two sample references:  @Feynman1963118 @Dirac1953888.

By default, natbib will be used with the `authoryear` style, set in `classoption` variable in YAML. 
You can sets extra options with `natbiboptions` variable in YAML header. Example 

```
natbiboptions: longnamesfirst,angle,semicolon
```

There are various more specific bibliography styles available at
<https://support.stmdocs.in/wiki/index.php?title=Model-wise_bibliographic_style_files>. 
To use one of these, add it in the header using, for example, `biblio-style: model1-num-names`.


## Using CSL 

If `cite-method` is set to `citeproc` in `elsevier_article()`, then pandoc is used for citations instead of `natbib`. In this case, the `csl` option is used to format the references. By default, this template will provide an appropriate style, but alternative `csl` files are available from <https://www.zotero.org/styles?q=elsevier>. These can be downloaded
and stored locally, or the url can be used as in the example header.